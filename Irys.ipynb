rom sklearn.tree import export_graphviz
import graphviz
import numpy as np
import math
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Wczytywanie danych
data = datasets.load_iris()

# Tworzenie DataFrame
df = pd.DataFrame(data.data, columns=data.feature_names)

# Dolączenie  kolumny z etykietami
df['target'] = data.target

# Zamiana wartości numerycznych klas na nazwy
df['target'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

# Wyświetlenie pierwszych wierszy DataFrame
df.head()
# Wizualizacja danych
sns.pairplot(df, hue='target', markers=['o', 's', 'D'], diag_kind='kde')
plt.show()
print(df.columns)
# Obliczenie liczby próbek
class_counts = df['target'].value_counts()

from sklearn import datasets
data = datasets.load_iris()


from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import mutual_info_score
import numpy as np
import matplotlib.pyplot as plt

# Wczytanie danych Iris
data = datasets.load_iris()
X = data.data
y = data.target

# Podział danych na zbiór treningowy i testowy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Funkcja do obliczenia entropii
def entropy(y):
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return -np.sum(probabilities * np.log2(probabilities))

# Funkcja do obliczenia zysku informacyjnego
def information_gain(parent, splits):
    num_parent = len(parent)
    num_splits = np.sum([len(split) for split in splits])
    entropy_parent = entropy(parent)
    entropy_children = np.sum([(len(split) / num_splits) * entropy(split) for split in splits])
    return entropy_parent - entropy_children

# Wizualizacja danych
plt.scatter(X[:, 2], X[:, 3], c=y, cmap=plt.cm.Set1, edgecolor='k')
plt.xlabel('Petal width (cm)')
plt.ylabel('Petal length (cm)')
plt.title('Iris Dataset')
plt.show()

# Tworzenie klasyfikatora drzewa decyzyjnego
clf = DecisionTreeClassifier(criterion='entropy')

# Trening klasyfikatora na danych treningowych
clf.fit(X_train, y_train)

# Przewidywanie na danych testowych
y_pred = clf.predict(X_test)

# Obliczenie dokładności klasyfikacji
accuracy = accuracy_score(y_test, y_pred)
print("Dokładność klasyfikacji:", accuracy)

# Podział danych na dwie grupy na podstawie petal width (cm) = 0.5 i 0.38
split_a = X[:, 3] <= 0.5
split_b = X[:, 3] <= 0.38

# Obliczenie zysku informacyjnego
information_gain_a = information_gain(y, [y[split_a], y[~split_a]])
information_gain_b = information_gain(y, [y[split_b], y[~split_b]])

print("Zysk informacyjny dla podziału a):", information_gain_a)
print("Zysk informacyjny dla podziału b):", information_gain_b)
