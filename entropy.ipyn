from sklearn import datasets
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import graphviz
from sklearn.tree import DecisionTreeClassifier, export_graphviz, DecisionTreeClassifier

dane = datasets.load_iris()
     

irys= pd.DataFrame(dane.data, columns=dane.feature_names)
irys['species'] = dane.target_names[dane.target]
irys.head(5)
# Wizualizacja danych
sns.pairplot(irys, hue='species', markers=['o', 's', 'D'], diag_kind='kde')
plt.show()
print(irys.columns)
liczba = irys['species'].value_counts()
print(liczba)
prawdopodobienstwo = liczba / len(irys)
print(f"Prawdopodobieństwo każdego gatunku:{prawdopodobienstwo}")
#obliczanie entropii dla całego zestawu danych
def entropia(prawdopodobienstwo):
    wartosc = 0
    for x in prawdopodobienstwo:
        wartosc -= x * np.log2(x)
    return wartosc
wynik_entropii=entropia(prawdopodobienstwo)
print(f"Wynik entropii dla całego zestawu to: {wartosc}")
X = pd.DataFrame(dane.data, columns=dane.feature_names)
y = irys.species
def entropy(labels):
    classes, counts = np.unique(labels, return_counts=True)
    probabilities = counts / len(labels)
    entropy = -np.sum(probabilities * np.log2(probabilities))
    return entropy
def zysk_informacyjny(data, split_value, feature_index):
    total_entropy = entropy(data[:, -1])
    #podział danych
    left_data = data[data[:, feature_index] <= split_value]
    right_data = data[data[:, feature_index] > split_value]
    #entropie dla grupy
    left_entropy = entropy(left_data[:, -1])
    right_entropy = entropy(right_data[:, -1])
    #obliczenie zysku
    num_left = len(left_data)
    num_right = len(right_data)
    total_instances = num_left + num_right
    zysk_informacyjny = total_entropy - ((num_left / total_instances) * left_entropy + (num_right / total_instances) * right_entropy)

    return zysk_informacyjny
#zyski dla grupy na poziomie 0.5
split_value_a = 0.5
feature_index_a = 3
information_gain_a = zysk_informacyjny(np.column_stack((X, y)), split_value_a, feature_index_a)
print("Zysk informacyjny dla podziału na szerokość płatka (0.5):", information_gain_a)
#zyski dla grupy na poziomie 0.38
split_value_b = 0.38
feature_index_b = 3
information_gain_b = zysk_informacyjny(np.column_stack((X, y)), split_value_b, feature_index_b)
print("Zysk informacyjny dla podziału na szerokość płatka (0.38):", information_gain_b)
_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)
dt_predictions = dt_classifier.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_predictions)
print("Dokładność drzewa:", dt_accuracy)


rf_classifier = RandomForestClassifier(n_estimators=15, random_state=42)
rf_classifier.fit(X_train, y_train)
rf_predictions = rf_classifier.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print("Dokładność lasu :", rf_accuracy)
graph_data = export_graphviz(decision_tree, out_file=None,
                           feature_names=dane.feature_names,
                           class_names=dane.target_names,
                           filled=True, rounded=True,
                           special_characters=True)
graph = graphviz.Source(graph_data)
graph
